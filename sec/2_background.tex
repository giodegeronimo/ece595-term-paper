\section{Background}
\label{sec:background}

% 2.1 Generative Modeling via Transport and Diffusion
%    (1–2 paragraphs)
% 2.2 Transformer Architectures for Vision
%    (1–2 paragraphs)
% 2.3 Temporal Modeling for Video Understanding
%    (1–2 paragraphs)
% 2.4 Image Segmentation and Mask-Based Query Representations

%-------------------------------------------------------------------------
\subsection{Generative Modeling via Transport and Diffusion}

Define diffusion models as stochastic processes that gradually transform noise → data.
Introduce probability-flow ODEs as deterministic counterparts.
Explain the idea of straightening probability paths.
Prepares the reader for why Rectified Flow matters.

%-------------------------------------------------------------------------
\subsection{Transformer Architectures for Vision}
Brief K/Q/V attention refresher.
Why ViTs treat images as token grids.
How patch representations differ from convolutional inductive biases.
Enables understanding of NaViT and Mask2Former.

%-------------------------------------------------------------------------
\subsection{Temporal Modeling for Video Understanding}

Define the task: assigning an action label per video frame.
Contrast:
frame-based classification (weak for long-range structure)
temporal models (RNNs, TCNs, cross-attention token models—used in FACT)
Prepares reader for why FACT uses learned action tokens.

%-------------------------------------------------------------------------
\subsection{Image Segmentation and Mask-Based Query Representations}
Image segmentation assigns semantic labels to every pixel of an image. Semantic segmentation groups pixels by class, instance segmentation predicts a per-object mask and class label, and panoptic segmentation unifies both by providing instance masks for "things" while still labeling the remaining "stuff" (pixels).

Query-based segmentation instead learns a set of global queries that interact with pixel features through cross-attention rather than predicting classes directly from convolutional heads. DETR \cite{carion2020detr} introduced this idea for object detection by letting each query predict a bounding box, demonstrating that detection can be cast as set prediction with minimal post-processing. Mask2Former \cite{cheng2022mask2former} generalizes the approach: each mask query iteratively performs masked cross-attention over multi-scale image features, and the same query embedding produces both a class logit and a mask via per-pixel dot products. Because queries are task-agnostic, the decoder can reuse identical heads for semantic, instance, and panoptic segmentation, providing the unified mask-based representation detailed throughout the paper.
