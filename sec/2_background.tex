\section{Background}
\label{sec:background}

% 2.1 Generative Modeling via Transport and Diffusion
%    (1–2 paragraphs)
% 2.2 Transformer Architectures for Vision
%    (1–2 paragraphs)
% 2.3 Temporal Modeling for Video Understanding
%    (1–2 paragraphs)
% 2.4 Image Segmentation and Mask-Based Query Representations

%-------------------------------------------------------------------------
\subsection{Generative Modeling via Transport and Diffusion}

Define diffusion models as stochastic processes that gradually transform noise → data.
Introduce probability-flow ODEs as deterministic counterparts.
Explain the idea of straightening probability paths.
Prepares the reader for why Rectified Flow matters.

%-------------------------------------------------------------------------
\subsection{Transformer Architectures for Vision}
The transformer architecture, introduced in \cite{vaswani2017attention}, has become ubiquitous across various domains in machine learning. This is a sequence-to-sequence architecture that uses only attention:
\[
\text{Attention}(Q, K, V) = \mathrm{softmax}\!\left(\frac{QK^{\top}}{\sqrt{d_k}}\right)V
\]
Given that the attention mechanism is permutation invaraint, positonal embeddings are added to the token embeddings to encode relative and absolute position in the sequence. Transformers have revolutionized language models by allowing all tokens in a sequence to attend to each other. \cite{dosovitskiy2021vit} applied this architecture to images by treating patches as words, showing that this revolutionary design was not limited to only natural language processing tasks. Compared to standard convolutional inductive biases, ViTs do not assume locality or translation invariance. Instead, they learns all structure from data, including which regions interact.

Within the field of computer vision, it has been a long-standing standard practice to pad or resize images to a fixed shape before before inputting them into a model. This is largely motivated by the efficiency gains of batched computations as well as the fixed input/output shapes that standard CNNs follow. ViTs inherit this constraint: increasing resolution explodes the token count quadratically, and padding wastes both memory and FLOPs. Yet many tasks actually benefit from preserving native resolution (fine-grained cues, elongated aspect ratios, and the ability to pick cost-quality trade-offs), so trimming or padding every image can degrade accuracy. NaViT \cite{dehghani2023navit} challenges this resizing/padding convention by borrowing a concept called sequence packing from LLM training, where multiple examples can be packed into a sequence with attention masking to efficiently use compute in batched computations when sequences vary in length. They greedily pack variable-resolution images into shared sequences, apply per-image attention masks so tokens only see siblings from the same image, and use factorized positional embeddings to encode height and width separately. This keeps training efficient while still exposing the model to diverse aspect ratios.
%-------------------------------------------------------------------------
\subsection{Temporal Modeling for Video Understanding}

The task of action segmentation within temporal modeling involves assigning an action label to each frame in a video. There are two ways this has been accomplished: frame-based, and two-stage. Frame-based methods involve directly predicting action labels from all the frames. This can be done using convolutions or transformers, but both of these methods struggle to capture long-range temporal dependencies. Convolutions have a fixed window size, and transformers have high computational complexity for long sequences, resulting in methods using windowed temporal attention. In both of these cases, the model's ability for long-range temporal modeling remains limited. Two-stage methods involve first extracting frame features independently or with small window sizes. Next, a transformer with learnable action tokens as queries, and frame features as keys and values generates a sequence of action tokens. The frame features and action tokens are then combined with a chosen mechanism to yield predicted labels for each frame. Frame-Action Cross-Attention Temporal Modeling (FACT) \cite{lu2024fact} builds upon the two-stage architecture by running both stages in parallel and implementing bilateral cross-attention between branches. This allows the complimentary information in high-level action dependencies and low-level frame features to be utilized to refine each other, resulting in better temporal modeling.

%-------------------------------------------------------------------------
\subsection{Image Segmentation and Mask-Based Query Representations}
Image segmentation assigns semantic labels to every pixel of an image. Semantic segmentation groups pixels by class, instance segmentation predicts a per-object mask and class label, and panoptic segmentation unifies both by providing instance masks for ``things'' while still labeling the remaining ``stuff'' (pixels).

Query-based segmentation instead learns a set of global queries that interact with pixel features through cross-attention rather than predicting classes directly from convolutional heads. DETR \cite{carion2020detr} introduced this idea for object detection by letting each query predict a bounding box, demonstrating that detection can be cast as set prediction with minimal post-processing. Mask2Former \cite{cheng2022mask2former} generalizes the approach: each mask query iteratively performs masked cross-attention over multi-scale image features, and the same query embedding produces both a class logit and a mask via per-pixel dot products. Because queries are task-agnostic, the decoder can reuse identical heads for semantic, instance, and panoptic segmentation, providing the unified mask-based representation detailed throughout the paper.
