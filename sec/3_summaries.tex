\section{Paper Summaries}
\label{sec:summaries}

% One paragraph per paper. Factual, descriptive, no critique yet.

%-------------------------------------------------------------------------
\subsection{Rectified Flow (Liu et al., 2023)}

Rectified Flow \cite{liu2023rectifiedflow} tackles the sampling inefficiency in diffusion-style models by forcing the learned transport ODE to follow straight lines between $x_0 \sim \pi_0$ and $x_1 \sim \pi_1$. The velocity field is trained with a simple least-squares loss on the straight interpolation $x_t = (1 - t)x_0 + t x_1$, so the model directly targets straight trajectories rather than the curved paths typical of probability-flow ODEs. They also propose a reflow step which involves regenerating $(x_0, x_1)$ pairs from the current flow and refitting the same objective to straighten the path again. This process is only applied a handful of times since additional passes can accumulate estimation error. On CIFAR-10 they report that the first rectified flow, solved with the adaptive RK45 Runge–Kutta integrator, achieves a Fréchet Inception Distance (FID) of 2.58 (lower indicates better image quality) and recall of 0.57 (higher indicates more diversity), matching the fully sampled VP/sub-VP SDE baselines that require thousands of denoising steps. After a second reflow they distill the model to a one-step generator and still obtain FID 4.85 with recall 0.50, showing that straight transport paths can bridge the gap between one-shot and continuous-time samplers without giving up competitiveness on quality or diversity.

%-------------------------------------------------------------------------
\subsection{FACT (Lu and Elhamifar, 2024)}

In the task of action segmentation, FACT \cite{lu2024fact} builds upon the two-stage approach by parallelizing the stages and introducing bilateral cross-attention between the branches. These branches are the action branch and the frame branch. The authors claim that the communication between the branches allows them to use less expensive convolutions for the frame branch rather than transformers, as the important temporal relations can be captured in the transformer-based action branch. This reduces computational costs, as there are always fewer action tokens than image frames. In the first block of the model, frame features are extracted using the frame branch. In the action branch $M$ learnable action tokens are input into a transformer with cross-attention where the action tokens are queries and the frame features are keys and values. Then, the model contains $B$ update blocks. In each block, action tokens get updated via cross-attention with frame features and are then followed by a self-attention layer. Frame features subsequently get updated via cross-attention with action tokens, followed by another convolution. These update blocks repeat, MLPs are applied to each branch, and their per-frame predictions are averaged at the end. FACT trains with four losses: frame and action cross-entropy terms applied at every block, a cross-attention consistency loss that encourages matching frame/action pairs, and a smoothing loss to stabilize action boundaries. Using this pipeline, FACT outperforms prior work in frame accuracy and segmental F1 across the four standard action segmentation datasets.

It is worth noting that in the architecture described above, action tokens map one-to-one with frame segments even if the action is the same. The authors explore a one-to-many approach to reduce the amount of action tokens and thus self-attention overhead, but only apply it to one dataset. Furthermore, they explore the idea of using transcripts from videos to initialize action tokens, finding that this increases performance. For the purpose of this paper, I will only focus on the one-to-one variation of FACT without using transcripts. 
%-------------------------------------------------------------------------
\subsection{NaViT (Dehghani et al., 2023)}

The long-standing practice in computer vision has been to pad or resize every image to a fixed shape before feeding it into a model, which either wastes compute (padding) or throws away useful spatial cues (resizing). NaViT \cite{dehghani2023navit} instead makes ViTs resolution-agnostic by packing variable-resolution images into shared sequences. A greedy algorithm fills each sequence until a token budget is reached, unused slots (pad tokens) are minimal ($<2\%$ of tokens), and attention masks ensure each image's tokens only see one another. Factorized positional embeddings encode height and width separately so the model understands aspect ratios, and variable token dropping lets larger images shed more patches during pretraining to balance compute. Coupled with random resolution sampling, this recipe delivers better accuracy than a vanilla ViT at comparable compute while training up to 4x faster, showing that native-resolution training can be both efficient and effective. The authors summarize their findings as: (i) random resolution sampling slashes training cost, (ii) NaViT achieves strong accuracy over a broad resolution range so inference cost can be dialed post hoc, and (iii) fixed batch shapes from packing enable new knobs such as aspect-ratio-preserving sampling, variable token dropping, and adaptive computation.
%-------------------------------------------------------------------------
\subsection{Mask2Former (Cheng et al., 2022)}

Mask2Former \cite{cheng2022mask2former} shows that semantic, instance, and panoptic segmentation can all be framed as predicting $N$ binary masks with $N$ labels, so it learns a single architecture that does exactly that. Images pass through a CNN backbone and pixel decoder to produce multi-scale features, and a pool of learned mask queries runs through transformer decoder layers that see progressively higher-resolution image features. Each query outputs a class logit and a mask logit via a dot-product with the full-resolution features, so the same query embedding decides “what” and “where.” Masked attention keeps each query focused on its current region, while the multi-scale ladder helps recover tiny structures. The authors also sub-sample pixels when computing mask loss, which cuts memory overhead without hurting accuracy. Across COCO panoptic, ADE20K semantic, and Cityscapes instance benchmarks, this universal design matches or beats task-specific models, giving a single set of weights that works everywhere.
