{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from data import ImageDataset\n",
    "from model import ViTVelocity\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader\n",
    "from contextlib import nullcontext\n",
    "from utils import train_loop, sample_rectified_flow, make_packing_collate\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "data_folder = \"../../../cat/\"\n",
    "img_paths = []\n",
    "import os\n",
    "\n",
    "for file in os.listdir(data_folder):\n",
    "    if file.endswith('.png'):\n",
    "        img_paths.append(os.path.join(data_folder, file))\n",
    "\n",
    "DS_SIZE = None\n",
    "NUM_EPOCHS = 500000\n",
    "SAMPLE_INTERVAL = 10\n",
    "PRINT_INTERVAL = 1\n",
    "NOISE_STD = 1\n",
    "BATCH_SIZE = 64\n",
    "DEVICE = 'mps'\n",
    "\n",
    "if DS_SIZE is not None:\n",
    "    img_paths = img_paths[:DS_SIZE]\n",
    "\n",
    "transform = v2.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.Resize((256, 256)),\n",
    "])\n",
    "\n",
    "ds = ImageDataset(img_paths, transform=transform)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [ds[i] for i in range(6)]\n",
    "save_image(samples, '../../figures/cats_ds.png', nrow=2, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from navit_rf.sampling import sample_rectified_flow\n",
    "\n",
    "\n",
    "def compare_models_over_steps(\n",
    "    model_no_rectify,\n",
    "    model_rectify,\n",
    "    steps_list,\n",
    "    *,\n",
    "    device,\n",
    "    img_size,\n",
    "    noise_std=1.0,\n",
    "    solver=\"heun\",\n",
    "    row_labels=(\"no rectify\", \"1 rectify\"),\n",
    "    seed=69,\n",
    "    title_font=14,\n",
    "    row_font=14,\n",
    "):\n",
    "    \"\"\"\n",
    "    Sample two models on the same noise for each steps count and show a 2×N grid.\n",
    "\n",
    "    Args:\n",
    "        model_no_rectify / model_rectify: trained velocity models.\n",
    "        steps_list (List[int]): number of integration steps (one column per entry).\n",
    "        device: torch device.\n",
    "        img_size (int): base resolution for sampling (assumes square).\n",
    "        noise_std (float): init noise std.\n",
    "        solver (str): 'heun' or 'euler'.\n",
    "        row_labels (tuple): text labels for the two rows.\n",
    "        seed (int): manual seed to sync noise across models.\n",
    "        title_font (int): font size for the column titles (N=…).\n",
    "        row_font (int): font size for the row labels.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(\n",
    "        2, len(steps_list), figsize=(3 * len(steps_list), 6), squeeze=False\n",
    "    )\n",
    "\n",
    "    def prep(img):\n",
    "        img = img[0].detach().cpu()\n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "        return np.transpose(img.numpy(), (1, 2, 0))\n",
    "\n",
    "    for col, steps in enumerate(steps_list):\n",
    "        torch.manual_seed(seed)\n",
    "        out_no = sample_rectified_flow(\n",
    "            model_no_rectify,\n",
    "            n=1,\n",
    "            device=device,\n",
    "            img_size=img_size,\n",
    "            steps=steps,\n",
    "            noise_std=noise_std,\n",
    "            solver=solver,\n",
    "        )\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        out_rect = sample_rectified_flow(\n",
    "            model_rectify,\n",
    "            n=1,\n",
    "            device=device,\n",
    "            img_size=img_size,\n",
    "            steps=steps,\n",
    "            noise_std=noise_std,\n",
    "            solver=solver,\n",
    "        )\n",
    "\n",
    "        axes[0, col].imshow(prep(out_no))\n",
    "        axes[0, col].set_title(f\"N = {steps}\", fontsize=title_font)\n",
    "        axes[1, col].imshow(prep(out_rect))\n",
    "\n",
    "        for row in range(2):\n",
    "            axes[row, col].set_xticks([])\n",
    "            axes[row, col].set_yticks([])\n",
    "            for spine in axes[row, col].spines.values():\n",
    "                spine.set_visible(False)\n",
    "\n",
    "    axes[0, 0].set_ylabel(row_labels[0], fontsize=row_font, rotation=90)\n",
    "    axes[1, 0].set_ylabel(row_labels[1], fontsize=row_font, rotation=90)\n",
    "    axes[0, 0].yaxis.set_label_coords(-0.12, 0.5)\n",
    "    axes[1, 0].yaxis.set_label_coords(-0.12, 0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import navit_rf as nrf\n",
    "\n",
    "model_no_rect = nrf.ViTVelocity(\n",
    "    patch=4,\n",
    "    in_ch=3,\n",
    "    d_model=512,\n",
    "    depth=8,\n",
    "    n_head=8,\n",
    "    mlp_ratio=4.0,\n",
    ")\n",
    "model_no_rect.load_state_dict(torch.load('model_2.pth', map_location='mps'))\n",
    "\n",
    "model_one_rect = nrf.ViTVelocity(\n",
    "    patch=4,\n",
    "    in_ch=3,\n",
    "    d_model=512,\n",
    "    depth=8,\n",
    "    n_head=8,\n",
    "    mlp_ratio=4.0,\n",
    ")\n",
    "model_one_rect.load_state_dict(torch.load('model_2_rect1.pth', map_location='mps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = compare_models_over_steps(\n",
    "    model_no_rect,\n",
    "    model_one_rect,\n",
    "    [0, 1, 2, 3, 5, 20, 50],\n",
    "    device='mps',\n",
    "    img_size=64,\n",
    "    noise_std=1.0,\n",
    "    solver=\"euler\",\n",
    "    row_labels=(\"no rectify\", \"1 rectify\"),\n",
    "    seed=69,\n",
    "    title_font=40,\n",
    "    row_font=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../../figures/0_vs_1rect_1.png\", dpi=300, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from navit_rf.sampling import sample_rectified_flow\n",
    "\n",
    "model = nrf.ViTVelocity(\n",
    "    patch=4,\n",
    "    in_ch=3,\n",
    "    d_model=512,\n",
    "    depth=8,\n",
    "    n_head=8,\n",
    "    mlp_ratio=4.0,\n",
    ")\n",
    "model.load_state_dict(torch.load('experiments/navit_rf/outputs/checkpoints/vit_velocity_epoch0235.pth', map_location='mps')['model'])\n",
    "\n",
    "sample = sample_rectified_flow(\n",
    "        model,\n",
    "        n=1,\n",
    "        device=DEVICE,\n",
    "        img_size=32,\n",
    "        steps=50,\n",
    "    )\n",
    "plt.imshow(sample.squeeze().permute(1,2,0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "SIZES = [\n",
    "    (32, 32),\n",
    "    (36, 36),\n",
    "    (40, 40),\n",
    "    (44, 44),\n",
    "    (48, 48),\n",
    "    (52, 52),\n",
    "    (60, 60),\n",
    "    (64, 64),\n",
    "]\n",
    "samples = []\n",
    "for i in range(len(SIZES)):\n",
    "    torch.random.manual_seed(69)\n",
    "\n",
    "    sample = sample_rectified_flow(\n",
    "        model,\n",
    "        n=1,\n",
    "        device=DEVICE,\n",
    "        img_size=SIZES[i][0],\n",
    "        steps=50,\n",
    "    )\n",
    "    sample = sample - sample.min()\n",
    "    sample /= sample.max()\n",
    "    sample = torch.nn.ZeroPad2d((0, 64-SIZES[i][0], 0, 64-SIZES[i][0]))(sample)\n",
    "    sample[sample==0]=1\n",
    "    samples.append(sample.squeeze())\n",
    "    print(sample.shape)\n",
    "grid_path = \"../../figures/NaViT_rf_samp1.png\"\n",
    "save_image(samples, grid_path, nrow=int(math.sqrt(len(SIZES))), normalize=False, padding=0)\n",
    "print(f\"Saved samples to {grid_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "SIZES = [\n",
    "    (32, 32),\n",
    "    (36, 36),\n",
    "    (40, 40),\n",
    "    (44, 44),\n",
    "    (48, 48),\n",
    "    (52, 52),\n",
    "    (60, 60),\n",
    "    (64, 64),\n",
    "]\n",
    "samples = []\n",
    "for i in range(len(SIZES)):\n",
    "    torch.random.manual_seed(42)\n",
    "\n",
    "    sample = sample_rectified_flow(\n",
    "        model,\n",
    "        n=1,\n",
    "        device=DEVICE,\n",
    "        img_size=SIZES[i][0],\n",
    "        steps=50,\n",
    "    )\n",
    "    sample = sample - sample.min()\n",
    "    sample /= sample.max()\n",
    "    sample = torch.nn.ZeroPad2d((0, 64-SIZES[i][0], 0, 64-SIZES[i][0]))(sample)\n",
    "    sample[sample==0]=1\n",
    "    samples.append(sample.squeeze())\n",
    "    print(sample.shape)\n",
    "grid_path = \"../../figures/NaViT_rf_samp2.png\"\n",
    "save_image(samples, grid_path, nrow=int(math.sqrt(len(SIZES))), normalize=False, padding=0)\n",
    "print(f\"Saved samples to {grid_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
